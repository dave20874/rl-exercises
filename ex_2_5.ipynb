{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex-2.5",
      "provenance": [],
      "authorship_tag": "ABX9TyMy9myzsaUiJeoz74OM4iR5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dave20874/rl-exercises/blob/main/ex_2_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgL6SYu7_0WY"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt \n",
        "import numpy import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook implements exercise 2.5 from Sutton and Barto's Reinforcement Learning.\n",
        "\n",
        "Our first class, Bandit, represents the non-stationary 10-Armed Bandit.  All the q_star(a) start out equal (at 0.0) and update with a random walk of mean 0, std dev 0.01.\n",
        "\n",
        "The update method updates the rewards on each time step.  The play method generates a random reward based on the players choice of action.  (play and update are independent so we can simulate agents with various epsilon-greedy behaviors together.)"
      ],
      "metadata": {
        "id": "UF_F2g4IB-yA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bandit:\n",
        "  REWARD_VARIANCE = 1.0\n",
        "  UPDATE_STD_DEV = 0.01\n",
        "\n",
        "  def __init__(self, num_actions=10):\n",
        "    # number of actions for this bandit\n",
        "    self.num_actions = num_actions\n",
        "\n",
        "    # Mean and std deviation for each action's reward\n",
        "    # self.mean_r[N] -> mean reward for action N.\n",
        "    # (Variance is 1.0 for all actions.  Std Dev = sqrt(variance) is also 1.)\n",
        "    self.mean_r = [0.0]*self.num_actions\n",
        "\n",
        "  # Return the ideal Q, q_star, for this bandit at this time.\n",
        "  def get_q_star(self):\n",
        "    return max([a[0] for a in self.distrib])\n",
        "\n",
        "  # return the best action for this bandit at this time.\n",
        "  def get_best_action(self):\n",
        "    means = [a[0] for a in self.distrib]\n",
        "    max = max(means)\n",
        "    return means.index(max)\n",
        "\n",
        "  # update distribution on each time step\n",
        "  def update(self):\n",
        "    for params in self.distrib:\n",
        "      params[0] += random.normal(0.0, self.UPDATE_STD_DEV)\n",
        "\n",
        "  # Play the game!  Get a reward!\n",
        "  def play(self, action):\n",
        "    mean = self.distrib[action][0]\n",
        "    r = random.normal(mean, self.REWARD_VARIANCE)\n",
        "    return r\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "LrQXh7MiCHTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next class defines the Agent.  It uses a parameter, alpha to set the step size and epsilon to set the learning rate.  An alpha value of 0.0 tells the agent to use sample averages instead of a constant step size."
      ],
      "metadata": {
        "id": "od_WTj91MEq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self, alpha, epsilon, num_actions=10):\n",
        "    self.alpha = alpha      # step size.  0.0 means use 1.0/self.n[a]\n",
        "    self.epsilon = epsilon  # Exploration rate\n",
        "    self.num_actions = num_actions\n",
        "    self.n = 0              # steps taken\n",
        "\n",
        "    # estimated Q values\n",
        "    self.q = [0.0]*self.num_actions\n",
        "    self.n = [0]*self.num_actions       # number of steps for this action\n",
        "\n",
        "  # Play one round with the bandit.\n",
        "  def play(self, bandit):\n",
        "    # Decide random or greedy action\n",
        "    action = self.get_greedy_action()\n",
        "    if random.uniform(0.0, 1.0) < self.epsilon:\n",
        "      # override greedy with random action\n",
        "      action = random.randint(0, self.num_actions)\n",
        "\n",
        "    # Get reward for this action\n",
        "    reward = bandit.play(action)\n",
        "\n",
        "    # Update q, n for this action\n",
        "    self.n[action] += 1\n",
        "    if self.alpha == 0.0:\n",
        "      # compute step size to give sample average\n",
        "      step_size = 1.0/self.n[action]\n",
        "    else:\n",
        "      # use alpha as step size\n",
        "      step_size = self.alpha\n",
        "\n",
        "    self.q[action] += step_size*(reward-self.q[action])\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xZ7kky6PM8-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use our Bandit and Agent to set up an experiment.\n"
      ],
      "metadata": {
        "id": "8ou_UNmsR2QF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Experiment:\n",
        "  RUNS = 10000\n",
        "  NUM_ACTIONS = 10\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def do_runs(self):\n",
        "    for n in range(self.RUNS):\n",
        "      self.run()\n",
        "\n",
        "  def run(self):\n",
        "    # create bandit\n",
        "    bandit = Bandit(self.NUM_ACTONS)\n",
        "\n",
        "    # create agents with different exploration rates\n",
        "    epsilon = 0.1\n",
        "    avg_agent = Agent(0.0, epsilon, self.NUM_ACTIONS)\n",
        "    const_sz_agent = Agent(0.1, epsilon, self.NUM_ACTIONS)\n",
        "\n",
        "    # go through the steps\n",
        "    for step in range(self.STEPS):\n",
        "      bandit.update()\n",
        "      avg_agent.play(bandit)\n",
        "      const_sz_agent.play(bandit)\n"
      ],
      "metadata": {
        "id": "P0QJl7mZR7_d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}