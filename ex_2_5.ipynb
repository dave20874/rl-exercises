{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex-2.5",
      "provenance": [],
      "authorship_tag": "ABX9TyN4Is06GLbEKX0m3gq2ewz6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dave20874/rl-exercises/blob/main/ex_2_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgL6SYu7_0WY"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt \n",
        "import numpy import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook implements exercise 2.5 from Sutton and Barto's Reinforcement Learning.\n",
        "\n",
        "Our first class, Bandit, represents the non-stationary 10-Armed Bandit.  All the q_star(a) start out equal (at 0.0) and update with a random walk of mean 0, std dev 0.01.\n",
        "\n",
        "The update method updates the rewards on each time step.  The play method generates a random reward based on the players choice of action.  (play and update are independent so we can simulate agents with various epsilon-greedy behaviors together.)"
      ],
      "metadata": {
        "id": "UF_F2g4IB-yA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bandit:\n",
        "  REWARD_VARIANCE = 1.0\n",
        "  UPDATE_STD_DEV = 0.01\n",
        "\n",
        "  def __init__(self, num_actions=10):\n",
        "    # number of actions for this bandit\n",
        "    self.num_actions = num_actions\n",
        "\n",
        "    # Mean and std deviation for each action's reward\n",
        "    # self.mean_r[N] -> mean reward for action N.\n",
        "    # (Variance is 1.0 for all actions.  Std Dev = sqrt(variance) is also 1.)\n",
        "    self.mean_r = [0.0]*self.num_actions\n",
        "\n",
        "  # Return the ideal Q, q_star, for this bandit at this time.\n",
        "  def get_q_star(self):\n",
        "    return max([a[0] for a in self.distrib])\n",
        "\n",
        "  # return the best action for this bandit at this time.\n",
        "  def get_best_action(self):\n",
        "    means = [a[0] for a in self.distrib]\n",
        "    max = max(means)\n",
        "    return means.index(max)\n",
        "\n",
        "  # update distribution on each time step\n",
        "  def update(self):\n",
        "    for params in self.distrib:\n",
        "      params[0] += random.normal(0.0, self.UPDATE_STD_DEV)\n",
        "\n",
        "  # Play the game!  Get a reward!\n",
        "  def play(self, action):\n",
        "    mean = self.distrib[action][0]\n",
        "    r = random.normal(mean, self.REWARD_VARIANCE)\n",
        "    return r\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "LrQXh7MiCHTO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}