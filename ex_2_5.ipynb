{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex-2.5",
      "provenance": [],
      "authorship_tag": "ABX9TyPPjOO/rcwZtT8awRCZewEd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dave20874/rl-exercises/blob/main/ex_2_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VgL6SYu7_0WY"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt \n",
        "from numpy import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook implements exercise 2.5 from Sutton and Barto's Reinforcement Learning.\n",
        "\n",
        "Our first class, Bandit, represents the non-stationary 10-Armed Bandit.  All the q_star(a) start out equal (at 0.0) and update with a random walk of mean 0, std dev 0.01.\n",
        "\n",
        "The update method updates the rewards on each time step.  The play method generates a random reward based on the players choice of action.  (play and update are independent so we can simulate agents with various epsilon-greedy behaviors together.)"
      ],
      "metadata": {
        "id": "UF_F2g4IB-yA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bandit:\n",
        "  REWARD_VARIANCE = 1.0\n",
        "  UPDATE_STD_DEV = 0.01\n",
        "\n",
        "  def __init__(self, num_actions=10):\n",
        "    # number of actions for this bandit\n",
        "    self.num_actions = num_actions\n",
        "\n",
        "    # Mean and std deviation for each action's reward\n",
        "    # self.mean_r[N] -> mean reward for action N.\n",
        "    # (Variance is 1.0 for all actions.  Std Dev = sqrt(variance) is also 1.)\n",
        "    self.mean_r = [0.0]*self.num_actions\n",
        "\n",
        "  # Return the ideal Q, q_star, for this bandit at this time.\n",
        "  def get_q_star(self):\n",
        "    return max([a[0] for a in self.distrib])\n",
        "\n",
        "  # return the best action for this bandit at this time.\n",
        "  def get_best_action(self):\n",
        "    means = [a[0] for a in self.distrib]\n",
        "    max = max(means)\n",
        "    return means.index(max)\n",
        "\n",
        "  # update distribution on each time step\n",
        "  def update(self):\n",
        "    for params in self.distrib:\n",
        "      params[0] += random.normal(0.0, self.UPDATE_STD_DEV)\n",
        "\n",
        "  # Play the game!  Get a reward!\n",
        "  def play(self, action):\n",
        "    mean = self.mean_r[action]\n",
        "    r = random.normal(mean, self.REWARD_VARIANCE)\n",
        "    return r\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "LrQXh7MiCHTO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next class defines the Agent.  It uses a parameter, alpha to set the step size and epsilon to set the learning rate.  An alpha value of 0.0 tells the agent to use sample averages instead of a constant step size."
      ],
      "metadata": {
        "id": "od_WTj91MEq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self, alpha, epsilon, num_actions=10):\n",
        "    self.alpha = alpha      # step size.  0.0 means use 1.0/self.n[a]\n",
        "    self.epsilon = epsilon  # Exploration rate\n",
        "    self.num_actions = num_actions\n",
        "    self.n = 0              # steps taken\n",
        "\n",
        "    # estimated Q values\n",
        "    self.q = [0.0]*self.num_actions\n",
        "    self.n = [0]*self.num_actions       # number of steps for this action\n",
        "\n",
        "  # Play one round with the bandit.\n",
        "  def play(self, bandit):\n",
        "    # Decide random or greedy action\n",
        "    action = self.get_greedy_action()\n",
        "    if random.uniform(0.0, 1.0) < self.epsilon:\n",
        "      # override greedy with random action\n",
        "      action = random.randint(0, self.num_actions)\n",
        "\n",
        "    # Get reward for this action\n",
        "    reward = bandit.play(action)\n",
        "\n",
        "    # Update q, n for this action\n",
        "    self.n[action] += 1\n",
        "    if self.alpha == 0.0:\n",
        "      # compute step size to give sample average\n",
        "      step_size = 1.0/self.n[action]\n",
        "    else:\n",
        "      # use alpha as step size\n",
        "      step_size = self.alpha\n",
        "\n",
        "    self.q[action] += step_size*(reward-self.q[action])\n",
        "\n",
        "    return reward\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xZ7kky6PM8-S"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use our Bandit and Agent to set up an experiment.\n"
      ],
      "metadata": {
        "id": "8ou_UNmsR2QF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Experiment:\n",
        "  RUNS = 2\n",
        "  NUM_ACTIONS = 10\n",
        "  STEPS = 500\n",
        "  def __init__(self):\n",
        "    self.agent1_rewards = [0.0]*self.STEPS\n",
        "    self.agent1_pct_opt = [0.0]*self.STEPS\n",
        "    self.agent2_rewards = [0.0]*self.STEPS\n",
        "    self.agent2_pct_opt = [0.0]*self.STEPS\n",
        "    self.optimal_reward = [0.0]*self.STEPS\n",
        "\n",
        "  def do_runs(self):\n",
        "    for n in range(self.RUNS):\n",
        "      self.run(n+1)\n",
        "\n",
        "  def run(self, run):\n",
        "    alpha = 1.0/run\n",
        "\n",
        "    # create bandit\n",
        "    bandit = Bandit(self.NUM_ACTIONS)\n",
        "\n",
        "    # create agents with different exploration rates\n",
        "    epsilon = 0.1\n",
        "    avg_agent = Agent(0.0, epsilon, self.NUM_ACTIONS)\n",
        "    const_sz_agent = Agent(0.1, epsilon, self.NUM_ACTIONS)\n",
        "\n",
        "    # go through the steps\n",
        "    for step in range(self.STEPS):\n",
        "      bandit.update()\n",
        "      optimal = bandit.get_q_star()\n",
        "      reward1 = avg_agent.play(bandit)\n",
        "      pct1 = reward1 / optimal\n",
        "      reward2 = const_sz_agent.play(bandit)\n",
        "      pct2 = reward2 / optimal\n",
        "\n",
        "      self.optimal_reward[step-1] += alpha*(optimal - self.optimal_reward[step-1])\n",
        "      self.agent1_rewards[step-1] += alpha*(reward1 - self.agent1_rewards[step-1])\n",
        "      self.agent1_pct_opt[step-1] += alpha*(pct1 - self.agent1_pct_opt[step-1])\n",
        "      self.agent2_rewards[step-1] += alpha*(reward2 - self.agent2_rewards[step-1])\n",
        "      self.agent2_pct_opt[step-1] += alpha*(pct2 - self.agent2_pct_opt[step-1])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P0QJl7mZR7_d"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp = Experiment()\n",
        "exp.do_runs()"
      ],
      "metadata": {
        "id": "72YEItPgWHbR",
        "outputId": "ae6b0abc-8ee8-4d55-b9c1-63926e5631fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-1175bdaeea20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_runs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-725a8d063f0a>\u001b[0m in \u001b[0;36mdo_runs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdo_runs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-725a8d063f0a>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, run)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# go through the steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m       \u001b[0mbandit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m       \u001b[0moptimal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbandit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_q_star\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0mreward1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbandit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-30a1841c5dc5>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;31m# update distribution on each time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistrib\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m       \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUPDATE_STD_DEV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Bandit' object has no attribute 'distrib'"
          ]
        }
      ]
    }
  ]
}